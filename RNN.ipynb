{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available.\n"
     ]
    }
   ],
   "source": [
    "#测试cuda是否可用\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available.\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#导入数据\n",
    "data_control = pd.read_excel('E:\\Huawei cloud\\华为云盘\\文档\\毕业论文\\stata代码\\data_control.xlsx')\n",
    "data_treat = pd.read_excel('E:\\Huawei cloud\\华为云盘\\文档\\毕业论文\\stata代码\\data_treat.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stkcd</th>\n",
       "      <th>Year</th>\n",
       "      <th>Vc</th>\n",
       "      <th>IND</th>\n",
       "      <th>Caperc</th>\n",
       "      <th>Lev</th>\n",
       "      <th>PPEperc</th>\n",
       "      <th>NPGR</th>\n",
       "      <th>SGR</th>\n",
       "      <th>RTR</th>\n",
       "      <th>ROA</th>\n",
       "      <th>Lnpatent</th>\n",
       "      <th>Lnsize</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>2004</td>\n",
       "      <td>False</td>\n",
       "      <td>建筑材料</td>\n",
       "      <td>0.115335</td>\n",
       "      <td>0.481675</td>\n",
       "      <td>0.982086</td>\n",
       "      <td>0.698250</td>\n",
       "      <td>0.088428</td>\n",
       "      <td>9.292599</td>\n",
       "      <td>0.089352</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>22.279432</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>2005</td>\n",
       "      <td>False</td>\n",
       "      <td>建筑材料</td>\n",
       "      <td>0.105231</td>\n",
       "      <td>0.508313</td>\n",
       "      <td>0.985549</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>0.077745</td>\n",
       "      <td>8.583376</td>\n",
       "      <td>0.073790</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>22.432246</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>2006</td>\n",
       "      <td>False</td>\n",
       "      <td>建筑材料</td>\n",
       "      <td>0.082842</td>\n",
       "      <td>0.565768</td>\n",
       "      <td>0.978328</td>\n",
       "      <td>0.092974</td>\n",
       "      <td>-0.014432</td>\n",
       "      <td>9.299277</td>\n",
       "      <td>0.067324</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>22.636097</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>2007</td>\n",
       "      <td>False</td>\n",
       "      <td>建筑材料</td>\n",
       "      <td>0.455311</td>\n",
       "      <td>0.493006</td>\n",
       "      <td>0.964869</td>\n",
       "      <td>0.370526</td>\n",
       "      <td>0.099847</td>\n",
       "      <td>13.703556</td>\n",
       "      <td>0.074495</td>\n",
       "      <td>2.890372</td>\n",
       "      <td>22.857785</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>False</td>\n",
       "      <td>建筑材料</td>\n",
       "      <td>0.123064</td>\n",
       "      <td>0.536142</td>\n",
       "      <td>0.970028</td>\n",
       "      <td>-0.118153</td>\n",
       "      <td>0.084869</td>\n",
       "      <td>15.531408</td>\n",
       "      <td>0.053134</td>\n",
       "      <td>2.197225</td>\n",
       "      <td>23.062767</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stkcd  Year     Vc   IND    Caperc       Lev   PPEperc      NPGR       SGR  \\\n",
       "0     12  2004  False  建筑材料  0.115335  0.481675  0.982086  0.698250  0.088428   \n",
       "1     12  2005  False  建筑材料  0.105231  0.508313  0.985549  0.026604  0.077745   \n",
       "2     12  2006  False  建筑材料  0.082842  0.565768  0.978328  0.092974 -0.014432   \n",
       "3     12  2007  False  建筑材料  0.455311  0.493006  0.964869  0.370526  0.099847   \n",
       "4     12  2008  False  建筑材料  0.123064  0.536142  0.970028 -0.118153  0.084869   \n",
       "\n",
       "         RTR       ROA  Lnpatent     Lnsize  action  \n",
       "0   9.292599  0.089352  0.693147  22.279432  2015.0  \n",
       "1   8.583376  0.073790  1.609438  22.432246  2015.0  \n",
       "2   9.299277  0.067324  1.945910  22.636097  2015.0  \n",
       "3  13.703556  0.074495  2.890372  22.857785  2015.0  \n",
       "4  15.531408  0.053134  2.197225  23.062767  2015.0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_treat['action'] = data_treat['Year'].where(data_treat['Vc']==True)\n",
    "data_treat['action'] = data_treat.groupby('Stkcd')['action'].transform('min')\n",
    "treat_pre = data_treat[data_treat['Year']<data_treat['action']]\n",
    "treat_post = data_treat[data_treat['Year']>=data_treat['action']]\n",
    "treat_pre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stkcd</th>\n",
       "      <th>Year</th>\n",
       "      <th>IND</th>\n",
       "      <th>Caperc</th>\n",
       "      <th>Lev</th>\n",
       "      <th>PPEperc</th>\n",
       "      <th>NPGR</th>\n",
       "      <th>SGR</th>\n",
       "      <th>RTR</th>\n",
       "      <th>ROA</th>\n",
       "      <th>Lnpatent</th>\n",
       "      <th>Lnsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.324336</td>\n",
       "      <td>0.594163</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.612774</td>\n",
       "      <td>0.099708</td>\n",
       "      <td>12.736188</td>\n",
       "      <td>0.069947</td>\n",
       "      <td>2.833213</td>\n",
       "      <td>23.466324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2005</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.361138</td>\n",
       "      <td>0.609809</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.570613</td>\n",
       "      <td>0.113548</td>\n",
       "      <td>10.997000</td>\n",
       "      <td>0.076395</td>\n",
       "      <td>1.609438</td>\n",
       "      <td>23.813962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.981768</td>\n",
       "      <td>0.649418</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.503140</td>\n",
       "      <td>0.096677</td>\n",
       "      <td>16.854687</td>\n",
       "      <td>0.061124</td>\n",
       "      <td>3.637586</td>\n",
       "      <td>24.604993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.943422</td>\n",
       "      <td>0.661125</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.467931</td>\n",
       "      <td>0.158089</td>\n",
       "      <td>57.790683</td>\n",
       "      <td>0.070893</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>25.329380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.144431</td>\n",
       "      <td>0.674441</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.127434</td>\n",
       "      <td>0.117774</td>\n",
       "      <td>45.860878</td>\n",
       "      <td>0.042309</td>\n",
       "      <td>3.295837</td>\n",
       "      <td>25.504375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stkcd  Year  IND    Caperc       Lev  PPEperc      NPGR       SGR  \\\n",
       "0      2  2004  房地产  0.324336  0.594163      1.0  0.612774  0.099708   \n",
       "1      2  2005  房地产  0.361138  0.609809      1.0  0.570613  0.113548   \n",
       "2      2  2006  房地产  0.981768  0.649418      1.0  0.503140  0.096677   \n",
       "3      2  2007  房地产  0.943422  0.661125      1.0  1.467931  0.158089   \n",
       "4      2  2008  房地产  0.144431  0.674441      1.0 -0.127434  0.117774   \n",
       "\n",
       "         RTR       ROA  Lnpatent     Lnsize  \n",
       "0  12.736188  0.069947  2.833213  23.466324  \n",
       "1  10.997000  0.076395  1.609438  23.813962  \n",
       "2  16.854687  0.061124  3.637586  24.604993  \n",
       "3  57.790683  0.070893  0.693147  25.329380  \n",
       "4  45.860878  0.042309  3.295837  25.504375  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "treat_pre = treat_pre.drop(['action'],axis=1)\n",
    "df = pd.concat([data_control,treat_pre],axis=0)\n",
    "df = df.drop(['Vc'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stkcd</th>\n",
       "      <th>Year</th>\n",
       "      <th>IND</th>\n",
       "      <th>Caperc</th>\n",
       "      <th>Lev</th>\n",
       "      <th>PPEperc</th>\n",
       "      <th>NPGR</th>\n",
       "      <th>SGR</th>\n",
       "      <th>RTR</th>\n",
       "      <th>ROA</th>\n",
       "      <th>Lnpatent</th>\n",
       "      <th>Lnsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2004</td>\n",
       "      <td>房地产</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>0.802780</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.007271</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>-0.029402</td>\n",
       "      <td>0.299939</td>\n",
       "      <td>0.151107</td>\n",
       "      <td>1.178013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2005</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.878237</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.007382</td>\n",
       "      <td>0.121104</td>\n",
       "      <td>-0.030076</td>\n",
       "      <td>0.409323</td>\n",
       "      <td>-0.653491</td>\n",
       "      <td>1.439538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.459028</td>\n",
       "      <td>1.069261</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.007559</td>\n",
       "      <td>0.085647</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>0.150264</td>\n",
       "      <td>0.679959</td>\n",
       "      <td>2.034625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>2007</td>\n",
       "      <td>房地产</td>\n",
       "      <td>0.431822</td>\n",
       "      <td>1.125721</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.005026</td>\n",
       "      <td>0.214715</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>-1.255927</td>\n",
       "      <td>2.579576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>房地产</td>\n",
       "      <td>-0.135054</td>\n",
       "      <td>1.189941</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.009214</td>\n",
       "      <td>0.129986</td>\n",
       "      <td>-0.016560</td>\n",
       "      <td>-0.168915</td>\n",
       "      <td>0.455269</td>\n",
       "      <td>2.711224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stkcd  Year  IND    Caperc       Lev   PPEperc      NPGR       SGR  \\\n",
       "0      2  2004  房地产 -0.007413  0.802780  0.779348 -0.007271  0.092017   \n",
       "1      2  2005  房地产  0.018697  0.878237  0.779348 -0.007382  0.121104   \n",
       "2      2  2006  房地产  0.459028  1.069261  0.779348 -0.007559  0.085647   \n",
       "3      2  2007  房地产  0.431822  1.125721  0.779348 -0.005026  0.214715   \n",
       "4      2  2008  房地产 -0.135054  1.189941  0.779348 -0.009214  0.129986   \n",
       "\n",
       "        RTR       ROA  Lnpatent    Lnsize  \n",
       "0 -0.029402  0.299939  0.151107  1.178013  \n",
       "1 -0.030076  0.409323 -0.653491  1.439538  \n",
       "2 -0.027805  0.150264  0.679959  2.034625  \n",
       "3 -0.011935  0.315987 -1.255927  2.579576  \n",
       "4 -0.016560 -0.168915  0.455269  2.711224  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "df_r = df[['Caperc','Lev','PPEperc','NPGR','SGR','RTR','ROA','Lnsize','Lnpatent']]\n",
    "scaler = StandardScaler().fit(df_r)\n",
    "df[['Caperc','Lev','PPEperc','NPGR','SGR','RTR','ROA','Lnsize','Lnpatent']] = scaler.transform(df_r)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stkcd</th>\n",
       "      <th>Year</th>\n",
       "      <th>IND</th>\n",
       "      <th>Caperc</th>\n",
       "      <th>Lev</th>\n",
       "      <th>PPEperc</th>\n",
       "      <th>NPGR</th>\n",
       "      <th>SGR</th>\n",
       "      <th>RTR</th>\n",
       "      <th>ROA</th>\n",
       "      <th>Lnpatent</th>\n",
       "      <th>Lnsize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.007413</td>\n",
       "      <td>0.802780</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.007271</td>\n",
       "      <td>0.092017</td>\n",
       "      <td>-0.029402</td>\n",
       "      <td>0.299939</td>\n",
       "      <td>0.151107</td>\n",
       "      <td>1.178013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>11</td>\n",
       "      <td>0.018697</td>\n",
       "      <td>0.878237</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.007382</td>\n",
       "      <td>0.121104</td>\n",
       "      <td>-0.030076</td>\n",
       "      <td>0.409323</td>\n",
       "      <td>-0.653491</td>\n",
       "      <td>1.439538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2006</td>\n",
       "      <td>11</td>\n",
       "      <td>0.459028</td>\n",
       "      <td>1.069261</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.007559</td>\n",
       "      <td>0.085647</td>\n",
       "      <td>-0.027805</td>\n",
       "      <td>0.150264</td>\n",
       "      <td>0.679959</td>\n",
       "      <td>2.034625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2007</td>\n",
       "      <td>11</td>\n",
       "      <td>0.431822</td>\n",
       "      <td>1.125721</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.005026</td>\n",
       "      <td>0.214715</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>0.315987</td>\n",
       "      <td>-1.255927</td>\n",
       "      <td>2.579576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.135054</td>\n",
       "      <td>1.189941</td>\n",
       "      <td>0.779348</td>\n",
       "      <td>-0.009214</td>\n",
       "      <td>0.129986</td>\n",
       "      <td>-0.016560</td>\n",
       "      <td>-0.168915</td>\n",
       "      <td>0.455269</td>\n",
       "      <td>2.711224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stkcd  Year  IND    Caperc       Lev   PPEperc      NPGR       SGR  \\\n",
       "0      0  2004   11 -0.007413  0.802780  0.779348 -0.007271  0.092017   \n",
       "1      0  2005   11  0.018697  0.878237  0.779348 -0.007382  0.121104   \n",
       "2      0  2006   11  0.459028  1.069261  0.779348 -0.007559  0.085647   \n",
       "3      0  2007   11  0.431822  1.125721  0.779348 -0.005026  0.214715   \n",
       "4      0  2008   11 -0.135054  1.189941  0.779348 -0.009214  0.129986   \n",
       "\n",
       "        RTR       ROA  Lnpatent    Lnsize  \n",
       "0 -0.029402  0.299939  0.151107  1.178013  \n",
       "1 -0.030076  0.409323 -0.653491  1.439538  \n",
       "2 -0.027805  0.150264  0.679959  2.034625  \n",
       "3 -0.011935  0.315987 -1.255927  2.579576  \n",
       "4 -0.016560 -0.168915  0.455269  2.711224  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder_stkcd = LabelEncoder()\n",
    "df['Stkcd'] = label_encoder_stkcd.fit_transform(df['Stkcd'])\n",
    "label_encoder_ind = LabelEncoder()\n",
    "df['IND']= label_encoder_ind.fit_transform(df['IND'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2321\n"
     ]
    }
   ],
   "source": [
    "num_stocks=df['Stkcd'].nunique()\n",
    "print(num_stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成时间序列数据集\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "Stkcd_features = ['Stkcd']\n",
    "IND_features = ['IND']\n",
    "dynamic_features = ['Caperc','Lev','PPEperc','NPGR','SGR','RTR','ROA','Lnsize','Lnpatent']\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, data, seq_length=3):\n",
    "        self.seq_length = seq_length\n",
    "        self.samples = []\n",
    "        \n",
    "        # 为每个股票生成所有可能的序列\n",
    "        for stkcd, group in data.groupby('Stkcd'):\n",
    "            group = group.sort_values('Year')\n",
    "            dynamic_values = group[dynamic_features].values.astype(np.float32)\n",
    "            Stkcd_values = group[Stkcd_features].iloc[0].values.astype(np.int64)\n",
    "            IND_values = group[IND_features].iloc[0].values.astype(np.int64)\n",
    "            \n",
    "            # 生成每个时间窗口的序列\n",
    "            for i in range(len(dynamic_values) - self.seq_length):\n",
    "                seq = dynamic_values[i:i+self.seq_length]\n",
    "                label = dynamic_values[i+self.seq_length][-1]  # 预测下一个时间点的 Lnpatent\n",
    "                self.samples.append({\n",
    "                    'stock_idx': Stkcd_values.item(),  # 提取标量值\n",
    "                    'IND_idx': IND_values.item(),      # 提取标量值\n",
    "                    'dynamic': seq,\n",
    "                    'label': label\n",
    "                })\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        return {\n",
    "            'stock_idx': torch.tensor(sample['stock_idx'], dtype=torch.long),  # 标量值\n",
    "            'IND_idx': torch.tensor(sample['IND_idx'], dtype=torch.long),      # 标量值\n",
    "            'dynamic': torch.tensor(sample['dynamic'], dtype=torch.float32),\n",
    "            'label': torch.tensor(sample['label'], dtype=torch.float32)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_idx: tensor([1997,  348])\n",
      "IND_idx: tensor([28, 13])\n",
      "dynamic: tensor([[[-0.0817,  2.4747,  0.7761, -0.0081,  0.2936, -0.0330, -0.6853,\n",
      "           5.4610, -1.2559],\n",
      "         [ 0.0451,  2.4112,  0.7760, -0.0085,  0.1332, -0.0324, -0.6859,\n",
      "           5.5619, -0.5336],\n",
      "         [-0.1631,  2.4190,  0.7762, -0.0087,  0.1222, -0.0335, -0.6980,\n",
      "           5.6541,  0.0688]],\n",
      "\n",
      "        [[-0.1820, -0.0895, -1.8300, -0.0085,  0.1776, -0.0323,  0.4435,\n",
      "          -0.9508, -0.8002],\n",
      "         [-0.1715,  0.0326, -1.0746, -0.0096,  0.0230, -0.0327, -0.0131,\n",
      "          -0.7082, -0.6535],\n",
      "         [-0.0237, -0.5277, -0.8762, -0.0081,  0.0328, -0.0328,  0.1886,\n",
      "          -0.6506, -0.5336]]])\n",
      "label: tensor([-1.2559, -1.2559])\n"
     ]
    }
   ],
   "source": [
    "# 创建数据集和数据加载器\n",
    "dataset = StockDataset(df, seq_length=3)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# 检查第一个批次\n",
    "sample_batch = next(iter(dataloader))\n",
    "print(f\"stock_idx: {sample_batch['stock_idx']}\")\n",
    "print(f\"IND_idx: {sample_batch['IND_idx']}\")\n",
    "print(f\"dynamic: {sample_batch['dynamic']}\")\n",
    "print(f\"label: {sample_batch['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本数: 7385\n",
      "验证集样本数: 1847\n",
      "测试集样本数: 2309\n"
     ]
    }
   ],
   "source": [
    "#划分训练集、验证集、测试集\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=1)\n",
    "print(f\"训练集样本数: {len(df_train)}\")\n",
    "print(f\"验证集样本数: {len(df_val)}\")\n",
    "print(f\"测试集样本数: {len(df_test)}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "train_dataset = StockDataset(df_train, seq_length=3)\n",
    "val_dataset = StockDataset(df_val, seq_length=3)\n",
    "test_dataset = StockDataset(df_test, seq_length=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建数据加载器\n",
    "batch_size = 32\n",
    "train_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_dataset, batch_size=batch_size,shuffle=False)\n",
    "test_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_stocks, num_industries, embedding_dim_stock, embedding_dim_industry, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # 嵌入层\n",
    "        self.stock_embedding = nn.Embedding(num_embeddings=num_stocks, embedding_dim=embedding_dim_stock)\n",
    "        self.industry_embedding = nn.Embedding(num_embeddings=num_industries, embedding_dim=embedding_dim_industry)\n",
    "        \n",
    "        # LSTM 层\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,  # 动态特征的维度\n",
    "            hidden_size=hidden_size,  # 隐藏层大小\n",
    "            num_layers=num_layers,    # LSTM 层数\n",
    "            batch_first=True          # 输入数据的形状为 (batch_size, seq_length, input_size)\n",
    "        )\n",
    "        \n",
    "        # ReLU 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # 中间连接层\n",
    "        self.fc1 = nn.Linear(hidden_size + embedding_dim_stock + embedding_dim_industry, 128)  # 中间层大小\n",
    "        self.fc2 = nn.Linear(128, output_size)  # 输出层\n",
    "    \n",
    "    def forward(self, stock_idx, industry_idx,dynamic):\n",
    "        \n",
    "        # 嵌入层处理静态特征\n",
    "        stock_embedded = self.stock_embedding(stock_idx)  # 形状: (batch_size, embedding_dim_stock)\n",
    "        industry_embedded = self.industry_embedding(industry_idx)  # 形状: (batch_size, embedding_dim_industry)\n",
    "        \n",
    "        # LSTM 处理动态特征\n",
    "        lstm_out, _ = self.lstm(dynamic)  # lstm_out 的形状: (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # 取最后一个时间步的输出\n",
    "        lstm_last_out = lstm_out[:, -1, :]  # lstm_last_out 的形状: (batch_size, hidden_size)\n",
    "        \n",
    "        # 添加 ReLU 激活函数\n",
    "        lstm_last_out = self.relu(lstm_last_out)\n",
    "        \n",
    "        # 将 LSTM 输出与嵌入后的静态特征结合\n",
    "        combined = torch.cat((lstm_last_out, stock_embedded, industry_embedded), dim=1)  # combined 的形状: (batch_size, hidden_size + embedding_dim_stock + embedding_dim_industry)\n",
    "        \n",
    "        # 通过中间连接层\n",
    "        fc1_out = self.relu(self.fc1(combined))  # 形状: (batch_size, 128)\n",
    "        \n",
    "        # 通过输出层（不加激活函数）\n",
    "        output = self.fc2(fc1_out)  # 形状: (batch_size, output_size)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (stock_embedding): Embedding(2321, 16)\n",
       "  (industry_embedding): Embedding(50, 8)\n",
       "  (lstm): LSTM(9, 64, num_layers=2, batch_first=True)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=88, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 声明模型参数\n",
    "model = LSTMModel(\n",
    "    input_size=9,  # 动态特征的维度\n",
    "    hidden_size=64,  # LSTM 隐藏层大小\n",
    "    num_layers=2,    # LSTM 层数\n",
    "    num_stocks=df['Stkcd'].nunique(),  # 股票代码的数量\n",
    "    num_industries=50,  # 行业分类的数量\n",
    "    embedding_dim_stock=16,  # 股票代码的嵌入维度\n",
    "    embedding_dim_industry=8,  # 行业分类的嵌入维度\n",
    "    output_size=1  # 输出维度\n",
    ")\n",
    "# 将模型移动到 GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()  # 均方误差损失\n",
    "import torch.optim as optim\n",
    "# 定义优化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # 学习率可以根据需要调整\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写train函数\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    \n",
    "    # 初始化损失值\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # 遍历数据加载器\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        # 从批次中提取数据\n",
    "        stock_idx = batch['stock_idx'].to(device).long()\n",
    "        industry_idx = batch['IND_idx'].to(device).long()\n",
    "        dynamic = batch['dynamic'].to(device)\n",
    "        label = batch['label'].to(device)\n",
    "\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 前向传播\n",
    "        output = model(stock_idx, industry_idx, dynamic)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        # 反向传播\n",
    "        loss.backward()\n",
    "        \n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 累加损失值\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = running_loss / len(train_dl)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#编写evaluate函数\n",
    "def evaluate(dataloader):\n",
    "    # 将模型设置为评估模式\n",
    "    model.eval()\n",
    "    \n",
    "    # 初始化损失值\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # 用于存储预测结果和真实值（可选）\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    \n",
    "    # 禁用梯度计算\n",
    "    with torch.no_grad():\n",
    "        # 遍历数据加载器\n",
    "        for batch_idx, batch in enumerate(val_dl):\n",
    "        # 从批次中提取数据\n",
    "            stock_idx = batch['stock_idx'].to(device)\n",
    "            industry_idx = batch['IND_idx'].to(device)\n",
    "            dynamic = batch['dynamic'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            \n",
    "            # 前向传播\n",
    "            output = model(stock_idx, industry_idx, dynamic)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = criterion(output, label)\n",
    "            \n",
    "            # 累加损失值\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # 记录预测结果和真实值（可选）\n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            labels.extend(label.cpu().numpy())\n",
    "    \n",
    "    # 计算平均损失\n",
    "    avg_loss = running_loss / len(val_dl)\n",
    "    \n",
    "    # 返回平均损失和预测结果（可选）\n",
    "    return avg_loss, predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "def calculate_metrics(predictions, labels):\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "    mse = np.mean((predictions - labels) ** 2)\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10000, Train Loss: 0.8891\n",
      "Epoch 1/10000, Val Loss: 0.9934\n",
      "Epoch 2/10000, Train Loss: 0.8822\n",
      "Epoch 2/10000, Val Loss: 1.0200\n",
      "Epoch 3/10000, Train Loss: 0.8815\n",
      "Epoch 3/10000, Val Loss: 0.8238\n",
      "Epoch 4/10000, Train Loss: 0.8815\n",
      "Epoch 4/10000, Val Loss: 0.8472\n",
      "Epoch 5/10000, Train Loss: 0.8737\n",
      "Epoch 5/10000, Val Loss: 0.9075\n",
      "Epoch 6/10000, Train Loss: 0.8688\n",
      "Epoch 6/10000, Val Loss: 0.8489\n",
      "Epoch 7/10000, Train Loss: 0.8740\n",
      "Epoch 7/10000, Val Loss: 0.7943\n",
      "Epoch 8/10000, Train Loss: 0.8746\n",
      "Epoch 8/10000, Val Loss: 0.8093\n",
      "Epoch 9/10000, Train Loss: 0.8714\n",
      "Epoch 9/10000, Val Loss: 0.8683\n",
      "Epoch 10/10000, Train Loss: 0.9042\n",
      "Epoch 10/10000, Val Loss: 0.8498\n",
      "Epoch 11/10000, Train Loss: 0.8688\n",
      "Epoch 11/10000, Val Loss: 0.8697\n",
      "Epoch 12/10000, Train Loss: 0.8679\n",
      "Epoch 12/10000, Val Loss: 0.8377\n",
      "Epoch 13/10000, Train Loss: 0.8807\n",
      "Epoch 13/10000, Val Loss: 0.7841\n",
      "Epoch 14/10000, Train Loss: 0.8751\n",
      "Epoch 14/10000, Val Loss: 0.8247\n",
      "Epoch 15/10000, Train Loss: 0.8861\n",
      "Epoch 15/10000, Val Loss: 0.8594\n",
      "Epoch 16/10000, Train Loss: 0.8690\n",
      "Epoch 16/10000, Val Loss: 0.8155\n",
      "Epoch 17/10000, Train Loss: 0.8722\n",
      "Epoch 17/10000, Val Loss: 0.8779\n",
      "Epoch 18/10000, Train Loss: 0.8705\n",
      "Epoch 18/10000, Val Loss: 0.8365\n",
      "Epoch 19/10000, Train Loss: 0.8892\n",
      "Epoch 19/10000, Val Loss: 0.8669\n",
      "Epoch 20/10000, Train Loss: 0.8742\n",
      "Epoch 20/10000, Val Loss: 0.8538\n",
      "Epoch 21/10000, Train Loss: 0.8742\n",
      "Epoch 21/10000, Val Loss: 0.8533\n",
      "Epoch 22/10000, Train Loss: 0.8738\n",
      "Epoch 22/10000, Val Loss: 0.8498\n",
      "Epoch 23/10000, Train Loss: 0.8708\n",
      "Epoch 23/10000, Val Loss: 0.8449\n",
      "Epoch 24/10000, Train Loss: 0.8754\n",
      "Epoch 24/10000, Val Loss: 0.8425\n",
      "Epoch 25/10000, Train Loss: 0.8754\n",
      "Epoch 25/10000, Val Loss: 0.8406\n",
      "Epoch 26/10000, Train Loss: 0.8693\n",
      "Epoch 26/10000, Val Loss: 0.8410\n",
      "Epoch 27/10000, Train Loss: 0.8755\n",
      "Epoch 27/10000, Val Loss: 0.8410\n",
      "Epoch 28/10000, Train Loss: 0.8762\n",
      "Epoch 28/10000, Val Loss: 0.8413\n",
      "Epoch 29/10000, Train Loss: 0.8738\n",
      "Epoch 29/10000, Val Loss: 0.8413\n",
      "Epoch 30/10000, Train Loss: 0.8751\n",
      "Epoch 30/10000, Val Loss: 0.8416\n",
      "Epoch 31/10000, Train Loss: 0.8700\n",
      "Epoch 31/10000, Val Loss: 0.8419\n",
      "Epoch 32/10000, Train Loss: 0.8798\n",
      "Epoch 32/10000, Val Loss: 0.8419\n",
      "Epoch 33/10000, Train Loss: 0.8703\n",
      "Epoch 33/10000, Val Loss: 0.8419\n",
      "Epoch 34/10000, Train Loss: 0.8720\n",
      "Epoch 34/10000, Val Loss: 0.8419\n",
      "Epoch 35/10000, Train Loss: 0.8692\n",
      "Epoch 35/10000, Val Loss: 0.8419\n",
      "Epoch 36/10000, Train Loss: 0.8872\n",
      "Epoch 36/10000, Val Loss: 0.8419\n",
      "Epoch 37/10000, Train Loss: 0.8726\n",
      "Epoch 37/10000, Val Loss: 0.8419\n",
      "Epoch 38/10000, Train Loss: 0.8798\n",
      "Epoch 38/10000, Val Loss: 0.8419\n",
      "Epoch 39/10000, Train Loss: 0.8751\n",
      "Epoch 39/10000, Val Loss: 0.8419\n",
      "Epoch 40/10000, Train Loss: 0.8682\n",
      "Epoch 40/10000, Val Loss: 0.8419\n",
      "Epoch 41/10000, Train Loss: 0.8894\n",
      "Epoch 41/10000, Val Loss: 0.8419\n",
      "Epoch 42/10000, Train Loss: 0.8763\n",
      "Epoch 42/10000, Val Loss: 0.8419\n",
      "Epoch 43/10000, Train Loss: 0.8715\n",
      "Epoch 43/10000, Val Loss: 0.8419\n",
      "Epoch 44/10000, Train Loss: 0.8895\n",
      "Epoch 44/10000, Val Loss: 0.8419\n",
      "Epoch 45/10000, Train Loss: 0.8805\n",
      "Epoch 45/10000, Val Loss: 0.8419\n",
      "Epoch 46/10000, Train Loss: 0.8778\n",
      "Epoch 46/10000, Val Loss: 0.8419\n",
      "Epoch 47/10000, Train Loss: 0.8716\n",
      "Epoch 47/10000, Val Loss: 0.8419\n",
      "Epoch 48/10000, Train Loss: 0.8833\n",
      "Epoch 48/10000, Val Loss: 0.8419\n",
      "Epoch 49/10000, Train Loss: 0.8795\n",
      "Epoch 49/10000, Val Loss: 0.8419\n",
      "Epoch 50/10000, Train Loss: 0.8773\n",
      "Epoch 50/10000, Val Loss: 0.8419\n",
      "Epoch 51/10000, Train Loss: 0.8723\n",
      "Epoch 51/10000, Val Loss: 0.8419\n",
      "Epoch 52/10000, Train Loss: 0.8768\n",
      "Epoch 52/10000, Val Loss: 0.8419\n",
      "Epoch 53/10000, Train Loss: 0.8713\n",
      "Epoch 53/10000, Val Loss: 0.8419\n",
      "Epoch 54/10000, Train Loss: 0.8796\n",
      "Epoch 54/10000, Val Loss: 0.8419\n",
      "Epoch 55/10000, Train Loss: 0.8901\n",
      "Epoch 55/10000, Val Loss: 0.8419\n",
      "Epoch 56/10000, Train Loss: 0.8772\n",
      "Epoch 56/10000, Val Loss: 0.8419\n",
      "Epoch 57/10000, Train Loss: 0.8740\n",
      "Epoch 57/10000, Val Loss: 0.8419\n",
      "Epoch 58/10000, Train Loss: 0.8845\n",
      "Epoch 58/10000, Val Loss: 0.8419\n",
      "Epoch 59/10000, Train Loss: 0.8757\n",
      "Epoch 59/10000, Val Loss: 0.8419\n",
      "Epoch 60/10000, Train Loss: 0.8801\n",
      "Epoch 60/10000, Val Loss: 0.8419\n",
      "Epoch 61/10000, Train Loss: 0.8755\n",
      "Epoch 61/10000, Val Loss: 0.8419\n",
      "Epoch 62/10000, Train Loss: 0.8759\n",
      "Epoch 62/10000, Val Loss: 0.8419\n",
      "Epoch 63/10000, Train Loss: 0.8698\n",
      "Epoch 63/10000, Val Loss: 0.8419\n",
      "Epoch 64/10000, Train Loss: 0.8812\n",
      "Epoch 64/10000, Val Loss: 0.8419\n",
      "Epoch 65/10000, Train Loss: 0.8755\n",
      "Epoch 65/10000, Val Loss: 0.8419\n",
      "Epoch 66/10000, Train Loss: 0.8683\n",
      "Epoch 66/10000, Val Loss: 0.8419\n",
      "Epoch 67/10000, Train Loss: 0.8713\n",
      "Epoch 67/10000, Val Loss: 0.8419\n",
      "Epoch 68/10000, Train Loss: 0.8720\n",
      "Epoch 68/10000, Val Loss: 0.8419\n",
      "Epoch 69/10000, Train Loss: 0.8721\n",
      "Epoch 69/10000, Val Loss: 0.8419\n",
      "Epoch 70/10000, Train Loss: 0.8765\n",
      "Epoch 70/10000, Val Loss: 0.8419\n",
      "Epoch 71/10000, Train Loss: 0.8869\n",
      "Epoch 71/10000, Val Loss: 0.8419\n",
      "Epoch 72/10000, Train Loss: 0.8789\n",
      "Epoch 72/10000, Val Loss: 0.8419\n",
      "Epoch 73/10000, Train Loss: 0.8828\n",
      "Epoch 73/10000, Val Loss: 0.8419\n",
      "Epoch 74/10000, Train Loss: 0.8719\n",
      "Epoch 74/10000, Val Loss: 0.8419\n",
      "Epoch 75/10000, Train Loss: 0.8690\n",
      "Epoch 75/10000, Val Loss: 0.8419\n",
      "Epoch 76/10000, Train Loss: 0.8701\n",
      "Epoch 76/10000, Val Loss: 0.8419\n",
      "Epoch 77/10000, Train Loss: 0.8789\n",
      "Epoch 77/10000, Val Loss: 0.8419\n",
      "Epoch 78/10000, Train Loss: 0.8784\n",
      "Epoch 78/10000, Val Loss: 0.8419\n",
      "Epoch 79/10000, Train Loss: 0.8716\n",
      "Epoch 79/10000, Val Loss: 0.8419\n",
      "Epoch 80/10000, Train Loss: 0.8733\n",
      "Epoch 80/10000, Val Loss: 0.8419\n",
      "Epoch 81/10000, Train Loss: 0.8801\n",
      "Epoch 81/10000, Val Loss: 0.8419\n",
      "Epoch 82/10000, Train Loss: 0.8726\n",
      "Epoch 82/10000, Val Loss: 0.8419\n",
      "Epoch 83/10000, Train Loss: 0.8859\n",
      "Epoch 83/10000, Val Loss: 0.8419\n",
      "Epoch 84/10000, Train Loss: 0.8702\n",
      "Epoch 84/10000, Val Loss: 0.8419\n",
      "Epoch 85/10000, Train Loss: 0.8704\n",
      "Epoch 85/10000, Val Loss: 0.8419\n",
      "Epoch 86/10000, Train Loss: 0.8761\n",
      "Epoch 86/10000, Val Loss: 0.8419\n",
      "Epoch 87/10000, Train Loss: 0.8700\n",
      "Epoch 87/10000, Val Loss: 0.8419\n",
      "Epoch 88/10000, Train Loss: 0.8695\n",
      "Epoch 88/10000, Val Loss: 0.8419\n",
      "Epoch 89/10000, Train Loss: 0.8894\n",
      "Epoch 89/10000, Val Loss: 0.8419\n",
      "Epoch 90/10000, Train Loss: 0.8721\n",
      "Epoch 90/10000, Val Loss: 0.8419\n",
      "Epoch 91/10000, Train Loss: 0.8797\n",
      "Epoch 91/10000, Val Loss: 0.8419\n",
      "Epoch 92/10000, Train Loss: 0.8711\n",
      "Epoch 92/10000, Val Loss: 0.8419\n",
      "Epoch 93/10000, Train Loss: 0.8748\n",
      "Epoch 93/10000, Val Loss: 0.8419\n",
      "Epoch 94/10000, Train Loss: 0.8734\n",
      "Epoch 94/10000, Val Loss: 0.8419\n",
      "Epoch 95/10000, Train Loss: 0.8698\n",
      "Epoch 95/10000, Val Loss: 0.8419\n",
      "Epoch 96/10000, Train Loss: 0.8795\n",
      "Epoch 96/10000, Val Loss: 0.8419\n",
      "Epoch 97/10000, Train Loss: 0.8704\n",
      "Epoch 97/10000, Val Loss: 0.8419\n",
      "Epoch 98/10000, Train Loss: 0.8697\n",
      "Epoch 98/10000, Val Loss: 0.8419\n",
      "Epoch 99/10000, Train Loss: 0.8717\n",
      "Epoch 99/10000, Val Loss: 0.8419\n",
      "Epoch 100/10000, Train Loss: 0.8712\n",
      "Epoch 100/10000, Val Loss: 0.8419\n",
      "Epoch 101/10000, Train Loss: 0.8722\n",
      "Epoch 101/10000, Val Loss: 0.8419\n",
      "Epoch 102/10000, Train Loss: 0.8691\n",
      "Epoch 102/10000, Val Loss: 0.8419\n",
      "Epoch 103/10000, Train Loss: 0.8951\n",
      "Epoch 103/10000, Val Loss: 0.8419\n",
      "Epoch 104/10000, Train Loss: 0.8701\n",
      "Epoch 104/10000, Val Loss: 0.8419\n",
      "Epoch 105/10000, Train Loss: 0.8863\n",
      "Epoch 105/10000, Val Loss: 0.8419\n",
      "Epoch 106/10000, Train Loss: 0.8831\n",
      "Epoch 106/10000, Val Loss: 0.8419\n",
      "Epoch 107/10000, Train Loss: 0.8797\n",
      "Epoch 107/10000, Val Loss: 0.8419\n",
      "Epoch 108/10000, Train Loss: 0.8775\n",
      "Epoch 108/10000, Val Loss: 0.8419\n",
      "Epoch 109/10000, Train Loss: 0.8683\n",
      "Epoch 109/10000, Val Loss: 0.8419\n",
      "Epoch 110/10000, Train Loss: 0.8750\n",
      "Epoch 110/10000, Val Loss: 0.8419\n",
      "Epoch 111/10000, Train Loss: 0.8762\n",
      "Epoch 111/10000, Val Loss: 0.8419\n",
      "Epoch 112/10000, Train Loss: 0.8728\n",
      "Epoch 112/10000, Val Loss: 0.8419\n",
      "Epoch 113/10000, Train Loss: 0.8810\n",
      "Epoch 113/10000, Val Loss: 0.8419\n",
      "Epoch 114/10000, Train Loss: 0.8684\n",
      "Epoch 114/10000, Val Loss: 0.8419\n",
      "Epoch 115/10000, Train Loss: 0.8702\n",
      "Epoch 115/10000, Val Loss: 0.8419\n",
      "Epoch 116/10000, Train Loss: 0.8732\n",
      "Epoch 116/10000, Val Loss: 0.8419\n",
      "Epoch 117/10000, Train Loss: 0.8901\n",
      "Epoch 117/10000, Val Loss: 0.8419\n",
      "Epoch 118/10000, Train Loss: 0.8715\n",
      "Epoch 118/10000, Val Loss: 0.8419\n",
      "Epoch 119/10000, Train Loss: 0.8794\n",
      "Epoch 119/10000, Val Loss: 0.8419\n",
      "Epoch 120/10000, Train Loss: 0.8732\n",
      "Epoch 120/10000, Val Loss: 0.8419\n",
      "Epoch 121/10000, Train Loss: 0.8689\n",
      "Epoch 121/10000, Val Loss: 0.8419\n",
      "Epoch 122/10000, Train Loss: 0.8760\n",
      "Epoch 122/10000, Val Loss: 0.8419\n",
      "Epoch 123/10000, Train Loss: 0.8937\n",
      "Epoch 123/10000, Val Loss: 0.8419\n",
      "Epoch 124/10000, Train Loss: 0.8970\n",
      "Epoch 124/10000, Val Loss: 0.8419\n",
      "Epoch 125/10000, Train Loss: 0.8715\n",
      "Epoch 125/10000, Val Loss: 0.8419\n",
      "Epoch 126/10000, Train Loss: 0.8834\n",
      "Epoch 126/10000, Val Loss: 0.8419\n",
      "Epoch 127/10000, Train Loss: 0.8835\n",
      "Epoch 127/10000, Val Loss: 0.8419\n",
      "Epoch 128/10000, Train Loss: 0.8687\n",
      "Epoch 128/10000, Val Loss: 0.8419\n",
      "Epoch 129/10000, Train Loss: 0.8818\n",
      "Epoch 129/10000, Val Loss: 0.8419\n",
      "Epoch 130/10000, Train Loss: 0.8779\n",
      "Epoch 130/10000, Val Loss: 0.8419\n",
      "Epoch 131/10000, Train Loss: 0.8751\n",
      "Epoch 131/10000, Val Loss: 0.8419\n",
      "Epoch 132/10000, Train Loss: 0.8895\n",
      "Epoch 132/10000, Val Loss: 0.8419\n",
      "Epoch 133/10000, Train Loss: 0.8727\n",
      "Epoch 133/10000, Val Loss: 0.8419\n",
      "Epoch 134/10000, Train Loss: 0.8711\n",
      "Epoch 134/10000, Val Loss: 0.8419\n",
      "Epoch 135/10000, Train Loss: 0.8706\n",
      "Epoch 135/10000, Val Loss: 0.8419\n",
      "Epoch 136/10000, Train Loss: 0.8673\n",
      "Epoch 136/10000, Val Loss: 0.8419\n",
      "Epoch 137/10000, Train Loss: 0.8863\n",
      "Epoch 137/10000, Val Loss: 0.8419\n",
      "Epoch 138/10000, Train Loss: 0.8717\n",
      "Epoch 138/10000, Val Loss: 0.8419\n",
      "Epoch 139/10000, Train Loss: 0.8774\n",
      "Epoch 139/10000, Val Loss: 0.8419\n",
      "Epoch 140/10000, Train Loss: 0.8712\n",
      "Epoch 140/10000, Val Loss: 0.8419\n",
      "Epoch 141/10000, Train Loss: 0.8714\n",
      "Epoch 141/10000, Val Loss: 0.8419\n",
      "Epoch 142/10000, Train Loss: 0.8778\n",
      "Epoch 142/10000, Val Loss: 0.8419\n",
      "Epoch 143/10000, Train Loss: 0.8720\n",
      "Epoch 143/10000, Val Loss: 0.8419\n",
      "Epoch 144/10000, Train Loss: 0.8713\n",
      "Epoch 144/10000, Val Loss: 0.8419\n",
      "Epoch 145/10000, Train Loss: 0.8999\n",
      "Epoch 145/10000, Val Loss: 0.8419\n",
      "Epoch 146/10000, Train Loss: 0.8930\n",
      "Epoch 146/10000, Val Loss: 0.8419\n",
      "Epoch 147/10000, Train Loss: 0.8976\n",
      "Epoch 147/10000, Val Loss: 0.8419\n",
      "Epoch 148/10000, Train Loss: 0.8741\n",
      "Epoch 148/10000, Val Loss: 0.8419\n",
      "Epoch 149/10000, Train Loss: 0.8689\n",
      "Epoch 149/10000, Val Loss: 0.8419\n",
      "Epoch 150/10000, Train Loss: 0.8677\n",
      "Epoch 150/10000, Val Loss: 0.8419\n",
      "Epoch 151/10000, Train Loss: 0.8841\n",
      "Epoch 151/10000, Val Loss: 0.8419\n",
      "Epoch 152/10000, Train Loss: 0.8769\n",
      "Epoch 152/10000, Val Loss: 0.8419\n",
      "Epoch 153/10000, Train Loss: 0.8813\n",
      "Epoch 153/10000, Val Loss: 0.8419\n",
      "Epoch 154/10000, Train Loss: 0.8842\n",
      "Epoch 154/10000, Val Loss: 0.8419\n",
      "Epoch 155/10000, Train Loss: 0.8761\n",
      "Epoch 155/10000, Val Loss: 0.8419\n",
      "Epoch 156/10000, Train Loss: 0.8882\n",
      "Epoch 156/10000, Val Loss: 0.8419\n",
      "Epoch 157/10000, Train Loss: 0.8834\n",
      "Epoch 157/10000, Val Loss: 0.8419\n",
      "Epoch 158/10000, Train Loss: 0.8716\n",
      "Epoch 158/10000, Val Loss: 0.8419\n",
      "Epoch 159/10000, Train Loss: 0.8709\n",
      "Epoch 159/10000, Val Loss: 0.8419\n",
      "Epoch 160/10000, Train Loss: 0.8850\n",
      "Epoch 160/10000, Val Loss: 0.8419\n",
      "Epoch 161/10000, Train Loss: 0.8778\n",
      "Epoch 161/10000, Val Loss: 0.8419\n",
      "Epoch 162/10000, Train Loss: 0.8691\n",
      "Epoch 162/10000, Val Loss: 0.8419\n",
      "Epoch 163/10000, Train Loss: 0.8832\n",
      "Epoch 163/10000, Val Loss: 0.8419\n",
      "Epoch 164/10000, Train Loss: 0.8945\n",
      "Epoch 164/10000, Val Loss: 0.8419\n",
      "Epoch 165/10000, Train Loss: 0.8799\n",
      "Epoch 165/10000, Val Loss: 0.8419\n",
      "Epoch 166/10000, Train Loss: 0.8752\n",
      "Epoch 166/10000, Val Loss: 0.8419\n",
      "Epoch 167/10000, Train Loss: 0.8761\n",
      "Epoch 167/10000, Val Loss: 0.8419\n",
      "Epoch 168/10000, Train Loss: 0.8857\n",
      "Epoch 168/10000, Val Loss: 0.8419\n",
      "Epoch 169/10000, Train Loss: 0.8798\n",
      "Epoch 169/10000, Val Loss: 0.8419\n",
      "Epoch 170/10000, Train Loss: 0.8778\n",
      "Epoch 170/10000, Val Loss: 0.8419\n",
      "Epoch 171/10000, Train Loss: 0.8781\n",
      "Epoch 171/10000, Val Loss: 0.8419\n",
      "Epoch 172/10000, Train Loss: 0.8803\n",
      "Epoch 172/10000, Val Loss: 0.8419\n",
      "Epoch 173/10000, Train Loss: 0.8942\n",
      "Epoch 173/10000, Val Loss: 0.8419\n",
      "Epoch 174/10000, Train Loss: 0.8710\n",
      "Epoch 174/10000, Val Loss: 0.8419\n",
      "Epoch 175/10000, Train Loss: 0.8952\n",
      "Epoch 175/10000, Val Loss: 0.8419\n",
      "Epoch 176/10000, Train Loss: 0.8805\n",
      "Epoch 176/10000, Val Loss: 0.8419\n",
      "Epoch 177/10000, Train Loss: 0.8854\n",
      "Epoch 177/10000, Val Loss: 0.8419\n",
      "Epoch 178/10000, Train Loss: 0.8814\n",
      "Epoch 178/10000, Val Loss: 0.8419\n",
      "Epoch 179/10000, Train Loss: 0.8783\n",
      "Epoch 179/10000, Val Loss: 0.8419\n",
      "Epoch 180/10000, Train Loss: 0.8703\n",
      "Epoch 180/10000, Val Loss: 0.8419\n",
      "Epoch 181/10000, Train Loss: 0.8683\n",
      "Epoch 181/10000, Val Loss: 0.8419\n",
      "Epoch 182/10000, Train Loss: 0.8686\n",
      "Epoch 182/10000, Val Loss: 0.8419\n",
      "Epoch 183/10000, Train Loss: 0.8780\n",
      "Epoch 183/10000, Val Loss: 0.8419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#开始训练\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 训练阶段\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 调用 train 函数\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# 验证阶段\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[73], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 遍历数据加载器\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# 从批次中提取数据\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     stock_idx \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstock_idx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     12\u001b[0m     industry_idx \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIND_idx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     13\u001b[0m     dynamic \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10000 \n",
    "torch.manual_seed(1)\n",
    "\n",
    "#开始训练\n",
    "for epoch in range(num_epochs):\n",
    "    # 训练阶段\n",
    "    train_loss = train(train_dl)  # 调用 train 函数\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # 验证阶段\n",
    "    val_loss, predictions, labels = evaluate(val_dl)  # 调用 evaluate 函数\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # 更新学习率\n",
    "    scheduler.step(val_loss)  # 根据验证损失调整学习率\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
